Saliency For NMT
================

This is a collection of implementations of saliency methods for seq2seq-attn. There are five methods implemented here:
 - First-derivative sensitivity analysis
 - Input erasure (inspired by [Li, et al](https://arxiv.org/pdf/1612.08220.pdf))
 - An algorithm similar to SmoothGrad ([Smilkov, et al.](https://arxiv.org/pdf/1706.03825.pdf))
    - Differs the magnitude of the size and number of perturbations, and in input modality
 - An algorithm similar to LIME ([Ribiero, et al.](https://arxiv.org/pdf/1602.04938.pdf))
    - Differs in that it does continuous perturbations of the input vectors, it does not use any distance-based kernel for its cost function, and it uses cross-validated LASSO rather than the proposed k-LASSO.
    - Note: this algorithm is very slow and can take up to a couple minutes to run on a single sentence; I recommended that you disable it if you are not using it.
 - Layerwise Relevance Propagation (method taken exactly from [Arras, et al.](https://arxiv.org/pdf/1706.07206.pdf))

Some examples of saliency in action from an English-Spanish translation network:
  - [a neuron that detects whether a token is inside parentheses](https://rawgithub.com/dabbler0/saliency/master/examples/paren-saliencies.html)
  - [a neuron that seems to detect whether a token is inside a negated clause](https://rawgithub.com/dabbler0/saliency/master/examples/negation-saliencies.html)

In these examples saliency was taken at the end of each table with respect to all of the prior tokens in the sentence. Each file contains 400 example sentence with saliency taken at a random token.

Some select examples from the negated clause neuron:

![Recognizing the word "not" activating](https://github.com/dabbler0/saliency/blob/master/examples/saliency-example-1.png?raw=true)
![Recognizing the word "excluding" activating](https://github.com/dabbler0/saliency/blob/master/examples/saliency-example-2.png?raw=true)
![Recognizing the word "as" and a comma stopping](https://github.com/dabbler0/saliency/blob/master/examples/saliency-example-3.png?raw=true)

Programmatic Usage
==================

To see an example of usage, see `example.lua`, which takes arguments `-model` (a seq2seq-attn model file), `-dict` (a seq2seq-attn src.dict file), and `-description` (an optional file, which should be of the type generated by `describe.lua` from [seq2seq-comparison](https://github.com/dabbler0/seq2seq-comparison) and is used to normalize activations to mean-0 stdev-1). It then takes input in stdin, alternating sentences to run saliency on and a neuron to examine in the saliencies that were just generated.

Usage and Visualization
=======================

To get a visualization of saliencies for all neurons, you'll first need to generate a file that contains saliencies for all the neurons over a sample set of sentences.

To do so, use the script `all-saliencies.lua`. You'll first need to create a model list. This is a text file that contains groups of three lines, each of which is a location of a model, a location of a description file (as generated by [describe.lua here](https://github.com/dabbler0/nmt-shared-information)), and a location of a source dictionary file:

```
/path/to/model-1_final.t7
/path/to/model-1.src.dict
/path/to/description-1.t7
/path/to/model-2_final.t7
/path/to/model-2.src.dict
/path/to/description-2.t7
```

And so on. Then invoke `all-saliencies.lua` like so:

```bash
th all-saliencies.lua -max_len 100 -model_list model_list.txt -src_file /path/to/samples.tok -out_file out.json
```

The arguments are:
  - `model_list`, the location of the model list file as above,
  - `src_file`, the location of the sample file (one sentence per line, space-separated tokens; as would be fed into `train.lua` in seq2seq-attn)
  - `out_file`, location to write the output to. The output will be in json format.

Note: this process consumes a lot of memory and takes a while.

Once you've generated all the saliencies, you can visualize them using `visualization-server.py`:

```bash
python visualization-server.py 8080 out.json 400
```

Where the single command-line argument is the generated file from `all-saliencies.lua`, and the server will run on port `8080`, which can be changed. You can then view a visualization by visiting `localhost:8080/?neuron=123` for whichever neuron you wish to inspect. The visualization will only show the first `400` sentences, which can be changed as well, but should usually be small for performance reasons, as rendering large numbers of sentences can be slow.

Automatic Neuron Descriptions
=============================

To run the automatic neuron description procedure, invoke `neuron-descriptions.py` like so:

```bash
python neuron-descriptions.py out.json descriptions.json
```

Where the first command-line argument is the generated file from `all-saliencies.lua` and the second command-line argument is the destination file in which to write the descriptions (in json format). Excerpts from the descriptions will also be printed while they are being generated.

Descriptions will consist of neurons, each associated with an "interpretability" score and a list of attended tokens, where each token-neuron pair is associated with a score for how much that neuron tends to care about that token when it appears.
